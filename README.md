# basic-RAG

This script implements a retrievalâ€‘augmented generation (RAG) pipeline using LangChain: it loads Wikipedia pages on a given topic with the WikipediaLoader document loaderÂ 
GitHub
, semantically chunks them with the experimental SemanticChunker backed by OpenAI embeddingsÂ 
Medium
, indexes those chunks in a FAISS vector store for fast similarity searchÂ 
GitHub
, and at query time retrieves relevant passages, stuffs them into a hubâ€‘pulled RAG prompt templateÂ 
GitHub
, then generates answers via the ChatOpenAI wrapper calling GPTâ€‘3.5â€‘TurboÂ 
Medium
.

ğŸ”‘ Features
Web scraping & parsing via WikipediaLoader (2 articles) and bs4.SoupStrainer to limit HTML parse scopeÂ 
crummy.com
.

Semantic chunking with SemanticChunker(OpenAIEmbeddings()) to split texts at meaning shiftsÂ 
Medium
.

Vector indexing using FAISS.from_documents(...) for local, efficient storage and retrievalÂ 
GitHub
.

RAG chain composition using RunnablePassthrough, a hubâ€‘pulled prompt (jclemens24/rag-prompt), and StrOutputParser to condition GPTâ€‘3.5â€‘Turbo answers on retrieved contextÂ 
GitHub
.

Answer generation through ChatOpenAI(model_name="gpt-3.5-turbo") for conversational-quality responsesÂ 
Medium
.

ğŸ› ï¸ Prerequisites
PythonÂ 3.8+ with venv support; create envs per the official guideÂ 
Python documentation
.

Dependencies installed via pip install -r requirements.txt, including langchain, langchain-openai, langchain-experimental, langchain_community, chromadb or faiss-cpuÂ 
GitHub
.

OpenAI API key stored in OPENAI_API_KEY and assigned to openai.api_key as recommended by the OpenAI Python libraryÂ 
GitHub
.

ğŸš€ Installation

# 1. Clone
git clone https://github.com/yourâ€‘org/rag_webqa.git
cd rag_webqa

# 2. Create & activate virtualenv
python -m venv .venv
source .venv/bin/activate    # Windows: .venv\Scripts\activate

# 3. Install deps
pip install -r requirements.txt
â–¶ï¸ Usage

# Ensure OPENAI_API_KEY is set:
export OPENAI_API_KEY="sk-â€¦"

# Run the RAG script:
python rag_webqa.py
Youâ€™ll see console logs as the loader, chunker, and vector store initialize, then a final printed answer.

ğŸ“‚ Directory Layout

.
â”œâ”€â”€ rag_webqa.py      # your RAG pipeline script  
â”œâ”€â”€ requirements.txt  # Python deps  
â””â”€â”€ README.md         # this documentation  
ğŸ” How It Works

Step	Component	Description
1. Load	WikipediaLoader	Fetches full articles for a query, limited by BeautifulSoupâ€™s SoupStrainer filterÂ 
crummy.com
.
2. Split	SemanticChunker(OpenAIEmbeddings())	Splits documents at semantic boundaries rather than fixed lengthsÂ 
Medium
.
3. Index	FAISS.from_documents(...)	Embeds chunks & stores vectors locally for similarity searchÂ 
GitHub
.
4. Retrieve	vectorstore.as_retriever()	Performs kâ€‘nearestâ€neighbors search over embeddings.
5. Prompt	hub.pull("jclemens24/rag-prompt")	Loads a communityâ€‘curated prompt template that â€œstuffsâ€ context + questionÂ 
GitHub
.
6. Answer	ChatOpenAI(model_name="gptâ€‘3.5â€‘turbo")	Generates the final answer conditioned on provided contextÂ 
Medium
.
ğŸ”§ Customization
Change topic by editing the query argument in WikipediaLoader.

Persist FAISS DB to disk by passing persist_directory="db" to FAISS.from_documents().

Adjust retrieval with retriever = vectorstore.as_retriever(search_kwargs={"k": 4}).

Swap models to gpt-4o or gpt-4 in ChatOpenAI(...).

Prompt engineering: fork or modify the RAG prompt on LangChain HubÂ 
GitHub
.

Error handling: extend the try/except blocks around OpenAI calls per production best practicesÂ 
Medium
.

ğŸŒ Environment Variables

Name	Purpose
OPENAI_API_KEY	Your OpenAI secret key for embeddingsÂ &Â chat calls.
âš ï¸ Limitations
Prototype quality: minimal error handling, no rateâ€‘limit backâ€‘off, and limited logging.

Singleâ€‘topic loader: only 2 Wikipedia pages by default; feel free to extend to multiâ€‘page sites.

Costs: each embedding + chat call consumes OpenAI credits.

ğŸ“œ License
Licensed under MIT. See LICENSE for details.

ğŸ™‹ Author
FelipeÂ Ortiz â€¢ @felipeortizh â€¢ fortiz.huerta@gmail.com
